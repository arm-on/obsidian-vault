![[backdoor-attack.png]]
## <font color="#00b0f0">Key Notes</font>

- The first backdoor attack in the machine learning systems
- Only around 50 poisoning samples, ASR of above 90%
- The first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process.
- Blended Backdoor (Baseline)

## <font color="#00b0f0">Method</font>

![[Pasted image 20251003190654.png]]
![[Pasted image 20251003190757.png]]
![[Pasted image 20251003190950.png]]
![[Pasted image 20251003191001.png]]
![[Pasted image 20251003191007.png]]
![[Pasted image 20251003191012.png]]
![[Pasted image 20251003191023.png]]
![[Pasted image 20251003191031.png]]
![[Pasted image 20251003191054.png]]
![[Pasted image 20251003191113.png]]
![[Pasted image 20251003191124.png]]
## <font color="#00b0f0">Results</font>

![[Pasted image 20251003191213.png]]

## <font color="#00b0f0">Original Paper</font>

![[1712.05526v1.pdf]]
test
